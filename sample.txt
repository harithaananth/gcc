		Install Java - 
		> sudo apt-get update
		> sudo apt-get install default-jdk
		> java -version
		Add hadoop User
		> sudo addgroup hadoop
		> sudo adduser --ingroup hadoop hduser
		Add hduser to sudo group 
		> su k <or> sudo -s
		> sudo adduser hduser sudo
		Install ssh
		> sudo apt-get install ssh
		check with
		> which ssh
		> which sshd
		shift to hduser
		> sudo su hduser
		generate key and add it to authorised keys
		> ssh-keygen -t rsa
		> cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
		check if localhost works 
		> ssh localhost
		Download the hadoop Package and extract it
		> wget https://archive.apache.org/dist/hadoop/core/hadoop-2.7.3/hadoop-2.7.3.tar.gz
		> tar xvzf hadoop-2.6.5.tar.gz
		Move this to /usr/local/hadoop
		> sudo mkdir -p /usr/local/hadoop
		> sudo mv * /usr/local/hadoop
		> sudo chown -R hduser:hadoop /usr/local/hadoop
		Find JAVA_HOME path
		> update-alternatives --config java {take only till /usr/lib/jvm/java-8-openjdk-amd64}
		Update Bashrc
		> sudo nano ~/.bashrc
			#HADOOP VARIABLES START
			export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
			export HADOOP_INSTALL=/usr/local/hadoop
			export PATH=$PATH:$HADOOP_INSTALL/bin
			export PATH=$PATH:$HADOOP_INSTALL/sbin
			export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
			export HADOOP_COMMON_HOME=$HADOOP_INSTALL
			export HADOOP_HDFS_HOME=$HADOOP_INSTALL
			export YARN_HOME=$HADOOP_INSTALL
			export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
			export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"
			#HADOOP VARIABLES END
		Change the hadoop-env.sh file
		> nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh
		add - 
			export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
	
		> sudo mkdir -p /app/hadoop/tmp
		> sudo chown hduser:hadoop /app/hadoop/tmp
		Now edit the /usr/local/hadoop/etc/hadoop/core-site.xml file so that it has 
			<configuration>
			 <property>
			  <name>hadoop.tmp.dir</name>
			  <value>/app/hadoop/tmp</value>
			 </property>
	
			 <property>
			  <name>fs.default.name</name>
			  <value>hdfs://localhost:54310</value>
			 </property>
			</configuration>
		it is xml so indent correctly
	
		rename mapred-site.xml.template file to mapred-site.xml
		> cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/loc
		Edit this to have 
			<configuration>
			 <property>
			  <name>mapred.job.tracker</name>
			  <value>localhost:54311</value>
			 </property>
			</configuration>
	
		Specify namenode and datanode directories
		> sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
		> sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode
		> sudo chown -R hduser:hadoop /usr/local/hadoop_store
	
		Edit the hdfs-site.xml file to have 
			<configuration>
			 <property>
			  <name>dfs.replication</name>
			  <value>1</value>
			 </property>
			 <property>
			   <name>dfs.namenode.name.dir</name>
			   <value>file:/usr/local/hadoop_store/hdfs/namenode</value>
			 </property>
			 <property>
			   <name>dfs.datanode.data.dir</name>
			   <value>file:/usr/local/hadoop_store/hdfs/datanode</value>
			 </property>
			</configuration>
	
		Now check if Hadoop is working with 
		> hadoop version
		Format the file system
		> hadoop namenode -format
		If required go to sbin folder before starting - cd /usr/local/hadoop/sbin
		> start-all.sh
		<or>
		> start-dfs.sh
		> start-yarn.sh
		Create a textfile having some words for input to wordcount 
		> hdfs dfs -mkdir -p /user/
		> hdfs dfs -mkdir -p /user/hadoop/
		> hdfs dfs -mkdir -p /user/hadoop/inputfiles
		> hdfs dfs -put mapreduce.txt /user/hadoop/inputfiles
		> hdfs dfs -put jarfile.jar /user/hadoop/inputfiles
		> hadoop jar jarfile.jar wordcount /user/hadoop/inputfiles/Mapreduce.txt /user/hadoop/inputfiles/output
		> hdfs dfs -l /user/hadoop/inputfiles/output
		display the one having 5 zeros in its name by 
		> hdfs dfs -cat ...